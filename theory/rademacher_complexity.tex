\documentclass[../toml]{subfiles}

\begin{document}
\chapter{Rademacher Complexity}

Until now, we've only covered cases when the \textit{Hypothesis Set} is finite. We will
now extend the discussion to infinite sets. A general idea consists of reducing the
infinite hypotheses to a finite set of hypotheses. The reduction generally relies on
some kind of complexity analysis and \textit{Rademacher Complexity} being one of
them will be the subject of this chapter.

\section{Empirical Rademacher Complexity}

\begin{definition}[Empirical Rademacher Complexity]
Let $G$ be a family of functions mapping from $Z$ to $[a,b]$ and $S = \{z_1,z_2,
\ldots,z_m\}$ a fixed sample of size $m$. Then, the empirical Rademacher Complexity
of $G$ with respect to the sample S is defined as
\begin{align}
\hat{\cursive{R}}_S(G) = \underset{\sigma}{E}\left[ \underset{g \in G}{sup} \frac{1}{m} \sum_{i=1}^{m} \sigma_i g(z_i) \right] \label{def:emp_rademacher_complexity}
\end{align}
where $\sigma_i$'s are independent uniform random variables taking values in
$\{+1,-1\}$. It can also be written in vectorized form as
\begin{align}
\hat{\cursive{R}}_S(G) = \underset{\sigma}{E}\left[ \underset{g \in G}{sup} \frac{\sigma.g_S}{m} \right]
\end{align}
\end{definition}

$G$ can also be interpreted as a family of loss functions associated with $H$. The dot
production can be intuitively seen as the correlation of a given function $g$ with
random noise $\sigma$ and measures how well the function class $G$ correlates
with the sample set $S$. Until now, all our analyses have been independent of the nature
of the distribution but now observe that richer the family $G$, the better it can correlate
with random noise on average.

\section{Rademacher Complexity}
\begin{definition}[Rademacher Complexity] \label{def:rademacher_complexity}
\begin{align}
\cursive{R}_m(G) = \underset{S \sim D^m}{E}[\hat{\cursive{R}}_S(G)]
\end{align}
\end{definition}
\begin{proof}
This should be equivalent to the arguments made in Theorem \ref{th:exp_emp_risk}.
\end{proof}

\begin{theorem} \label{th:rademacher_complexity}
Let $G$ be a family of functions mapping from $Z$ to $[0,1]$. Then for any $\delta>0$,
with probability at least $1-\delta$ each of the following holds for all $g \in G$
\begin{align}
E[g(z)] \leq \hat{E}_S[g(z)] + 2\cursive{R}_m(G) + \sqrt{\frac{log\frac{1}{\delta}}{2m}} \label{eq:th:rademacher_1} \\
E[g(z)] \leq \hat{E}_S[g(z)] + 2\hat{\cursive{R}}_S(G) + 3\sqrt{\frac{log\frac{2}{\delta}}{2m}} \label{eq:th:rademacher_2}
\end{align}
where $\hat{E}_S[g(z)] = \frac{1}{m} \sum_{i=1}^{m} g(z_i)$ is the empirical average over
sample set $S$.
\end{theorem}
\begin{proof}
Define a function $\Phi(S) = \underset{g \in G}{sup} E[g] - \hat{E}_S[g]$. Now consider
two sample sets $S$ and $S^\prime$ which differ only at one instance $z_m$ and $z_m^
\prime$. Then we have
\begin{align}
\Phi(S) - \Phi(S^\prime) &\leq \underset{g \in G}{sup} \hat{E}_S[g] - \hat{E}_{S^\prime}[g] \nonumber \\
&\leq \underset{g \in G}{sup} \frac{g(z_m) - g(z_m^\prime}{m}) \nonumber \\
&\leq \frac{1}{m}
\end{align}
Similarly $\Phi(S^\prime) - \Phi(S) \leq \frac{1}{m}$ and hence, $\left| \Phi(S) - \Phi(S^\prime)\right| \leq \frac{1}{m}$

Using \textit{McDiarmid's Inequality}, for any $\delta > 0$, with probability at least
$1-\frac{\delta}{2}$ and setting $\frac{\delta}{2} = e^{-2m\epsilon^2}$, the following holds
\begin{align}
\Phi(S) \leq \underset{S}{E}[\Phi(S)] + \sqrt{\frac{log\frac{2}{\delta}}{2m}} \label{eq:ramemacher_proof_phi}
\end{align}

We now bound the term $\underset{S}{E}[\Phi(S)]$
\begin{align}
\underset{S}{E}[\Phi(S)] &= \underset{S}{E}\left[\underset{g\in G}{sup} E[g] - \hat{E}_S[g]\right] \tag{by definition}\\
&= \underset{S}{E}\left[\underset{g\in G}{sup} \underset{S^\prime}{E} \left [\hat{E}_{S^\prime}[g] - \hat{E}_S[g] \right] \right] \tag{$S$ and $S^\prime$ independent, $E[g] = \underset{S^\prime}{E}\left[\hat{E}_{S^\prime}[g]\right]$} \\
&\leq \underset{S,S^\prime}{E}\left[ \underset{g\in G}{sup} \hat{E}_{S^\prime}[g] - \hat{E}_S[g] \right] \tag{by Jensen's Inequality} \\
&= \underset{S,S^\prime}{E}\left[ \underset{g\in G}{sup} \frac{1}{m} \sum_{i=1}^{m} g(z_i) - g(z_i^\prime) \right] \tag{by definition} \\
&= \underset{\sigma,S,S^\prime}{E}\left[ \underset{g\in G}{sup} \frac{1}{m} \sum_{i=1}^{m} \sigma_i ( g(z_i) - g(z_i^\prime)) \right] \tag{Rademacher variables don't affect overall expectation} \\
&\leq \underset{\sigma,S}{E}\left[ \underset{g\in G}{sup} \frac{1}{m} \sum_{i=1}^{m} \sigma_i g(z_i) \right] + \underset{\sigma,S^\prime}{E}\left[ \underset{g\in G}{sup} \frac{1}{m} \sum_{i=1}^{m} -\sigma_i g(z_i^\prime) \right] \tag{by sub-additivity of supremum} \\
&= 2\cursive{R}_m(G) \tag{$\sigma$ and $-\sigma$ are distributed the same way}
\end{align}

Putting this back in Equation \ref{eq:ramemacher_proof_phi}, we get the desired result
of Equation \ref{eq:th:rademacher_1}. Note that we have arrived at a result for
$\frac{\delta}{2}$ but that is trivially replaceable by $\delta$.

Again, by the same \textit{McDiarmid's Inequality} proposition, we can find a relation
between \textit{Rademacher Complexity} and its empirical variant by virtue of Definition
\ref{def:rademacher_complexity}. We observe that changing one point in sample $S$,
changes $\hat{\cursive{R}}_S(G)$ by at most $\frac{1}{m}$. As a result with probability
$1 - \frac{\delta}{2}$
\begin{align}
\cursive{R}_m(G) \leq \hat{\cursive{R}}_S(G) + \sqrt{\frac{log\frac{2}{\delta}}{2m}} \label{eq:emp_ramemacher_proof}
\end{align}

Combining Equations \ref{eq:ramemacher_proof_phi} and \ref{eq:emp_ramemacher_proof}
and using the Union Bound (one should now realize why we used $\frac{\delta}{2}$ instead
of $\delta$), we have with probability $1-\delta$,

\begin{align}
\Phi(S) \leq 2\hat{\cursive{R}}_S(G) + 3\sqrt{\frac{log\frac{2}{\delta}}{2m}}
\end{align}

\end{proof}

\section{Relation to Hypothesis Sets}

While we have established the new notion of Rademacher complexity, we still haven't
gone back to our original learning problem where we had an infinite Hypothesis Set
and needed to find \textit{generalization bounds}.

The following lemma provides a relation of the \textit{Rademacher Complexity} to the
\textit{Hypothesis Set} with binary loss function.

\begin{lemma} \label{lem:rademacher_hyposets}
Let H be a family of functions taking values in $\{+1,-1\}$ and let G be a family of loss
functions associated with $H$, $G = \{(x,y) \mapsto 1_{h(x)\neq y} : h \in H$. For any
sample set $S$ of elements in $\cursive{X} \times \{+1,-1\}$, let $S_{\cursive{X}}$ denote
its projection over $\cursive{X}$, then
\begin{align}
\hat{\cursive{R}}_S(G) = \frac{1}{2}\hat{\cursive{R}}_{S_{\cursive{X}}}(H)\\
\cursive{R}_m(G) = \frac{1}{2}\cursive{R}_m(H)
\end{align}
\end{lemma}
\begin{proof}
\begin{align}
\hat{\cursive{R}}_S(G) &= \underset{\sigma}{E}\left[ \underset{g \in G}{sup} \frac{1}{m} \sum_{i=1}^{m} \sigma_i 1_{h(x_i)\neq y_i} \right] \tag{by Definition \ref{def:emp_rademacher_complexity}} \\
&= \underset{\sigma}{E}\left[ \underset{g \in G}{sup} \frac{1}{m} \sum_{i=1}^{m} \sigma_i \frac{1 - y_ih(x_i)}{2} \right] \nonumber \\
&= \frac{1}{2} \underset{\sigma}{E}\left[ \underset{g \in G}{sup} \frac{1}{m} \sum_{i=1}^{m} -\sigma_i y_ih(x_i) \right] \nonumber \\
&= \frac{1}{2} \underset{\sigma}{E}\left[ \underset{g \in G}{sup} \frac{1}{m} \sum_{i=1}^{m} \sigma_i h(x_i) \right] \tag{$\sigma_i$ and $-\sigma_iy_i$ are distributed same way} \\
&= \frac{1}{2} \hat{\cursive{R}}_{S_{\cursive{X}}}(H)
\end{align}

Taking expectation over the complete sample set space both sides should give us the
second relation as well.
\end{proof}

\begin{theorem}[Rademacher Complexity Bounds - Binary Classification] \label{th:rademacher_bounds_binary_classification}
Let H be a family of functions taking values in $\{+1,-1\}$ and let $D$ be the distribution
over input space \cursive{X}. Then, for any $\delta>0$, with probability at least $1-\delta$,
over a sample $S$ of size $m$, the following holds for all $h \in H$.
\begin{align}
R(h) \leq \hat{R}(h) + \cursive{R}_m(H) + \sqrt{\frac{log\frac{1}{\delta}}{2m}} \\
R(h) \leq \hat{R}(h) + \hat{\cursive{R}}_S(H) + 3\sqrt{\frac{log\frac{2}{\delta}}{2m}}
\end{align}
\end{theorem}
\begin{proof}
This directly follows from Lemma \ref{lem:rademacher_hyposets} and Theorem
\ref{th:rademacher_complexity}.
\end{proof}

But again, the calculation of $\hat{R}(h)$ is a computationally hard problem for some cases.

\section{Growth Function}

\begin{definition}[Growth Function] \label{def:growth_fn}
A growth function $\Pi_H: \mathbb{N} \to \mathbb{N}$ for a hypothesis set $H$ is defined
as
\begin{align}
\forall m \in \mathbb{N}, \Pi_H(m) = \underset{\{x_1,x_2,\ldots,x_m\} \subseteq X}{max} \left| \{ \left( h(x_1),h(x_2),\ldots,h(x_m) \right) \} : h \in H \right|
\end{align}
\end{definition}

Intuitively, $\Pi_H(m)$ is the maximum number of distinct ways in which $m$ points can
be classified using set $H$. It should be easy to see that as the hypothesis set gets richer,
we have more ways of classifying the sample set. But as opposed to the
\textit{Rademacher Complexity} which relies on the distribution, this measure is purely
combinatorial.

\begin{theorem}[Massart's lemma] \label{th:massart}
Let $A \subseteq \mathbb{R}^m$ be a finite set, with $r = \underset{x \in A}{max} || x ||_2$,
then
\begin{align}
\underset{\sigma}{E}\left[ \underset{x \in A}{sup} \frac{1}{m} \sum_{i=1}^{m} \sigma_i x_i \right] \leq \frac{r \sqrt{2 log |A|}}{m}
\end{align}
where $\sigma_i$ are independent random variables taking values $\{+1,-1\}$.
\end{theorem}
\begin{proof}
For an arbitrary $t>0$, we will use \textit{Jensen's Inequality} and bound the supremum by
sum of all terms.
\begin{align}
e^{t\underset{\sigma}{E}\left[ \underset{x \in A}{sup} \sum_{i=1}^{m} \sigma_i x_i \right]}
&\leq \underset{\sigma}{E}\left[ e^{t\underset{x \in A}{sup} \sum_{i=1}^{m} \sigma_i x_i} \right] \tag{by \textit{Jensen's Inequality}} \\
&= \underset{\sigma}{E}\left[ \underset{x \in A}{sup} \text{ } e^{\sum_{i=1}^{m} t\sigma_i x_i} \right] \nonumber \\
&\leq \sum_{x \in A} \underset{\sigma}{E}\left[ \text{ } e^{\sum_{i=1}^{m} t\sigma_i x_i} \right] \tag{bounding supremum by sum of all terms} \\
&= \sum_{x \in A} \prod_{i=1}^{m} \underset{\sigma_i}{E}\left[ e^{t\sigma_i x_i} \right] \tag{by independence of Expectation} \\
&\leq \sum_{x \in A} \prod_{i=1}^{m}  e^{\frac{t^2 (2x_i)^2}{8}} \tag{by Hoeffding's Lemma} \\
&= \sum_{x \in A} e^{\frac{t^2}{2} \sum_{i=1}^{m} x_i^2} \nonumber \\
&\leq \sum_{x \in A} e^{\frac{t^2r^2}{2}} \tag{by definition of r} \\
&= |A| e^{\frac{t^2r^2}{2}} \tag{by definition of r} \\
\implies, \underset{\sigma}{E}\left[ \underset{x \in A}{sup} \sum_{i=1}^{m} \sigma_i x_i \right] &\leq \frac{log |A|}{t} + \frac{tr^2}{2} \label{eq:massart_proof_rhs}
\end{align}

When we minimize Equation \ref{eq:massart_proof_rhs}, we get $t = \frac{\sqrt{2log|A|}}{r}$.
Putting it back and dividing by $m$ gives us
\begin{align}
\underset{\sigma}{E}\left[ \underset{x \in A}{sup} \frac{1}{m} \sum_{i=1}^{m} \sigma_i x_i \right] \leq \frac{r \sqrt{2 log |A|}}{m}
\end{align}
\end{proof}

This leads us to an interesting corollary for Binary Hypotheses.

\begin{corollary} \label{corr:massart}
Let G be a family of functions taking values in $\{+1,-1\}$, the the following holds
\begin{align}
\cursive{R}_m(G) \leq \sqrt{\frac{2log\Pi_G(m)}{m}}
\end{align}
\end{corollary}
\begin{proof}
We will apply Theorem \ref{th:massart} which provides a bound for the
\textit{Empirical Rademacher Complexity}. Hence,
\begin{align}
\cursive{R}_m(G) &= \underset{S}{E} \left[ \underset{\sigma}{E}\left[ \underset{g \in G_{|S}}{sup} \frac{1}{m} \sum_{i=1}^{m} \sigma_i g(x_i) \right] \right] \tag{by definition of Rademacher Complexity} \\
&\leq \underset{S}{E} \left[ \frac{\sqrt{m} \sqrt{2 log |G_{|S}|}}{m} \right] \tag{by Massart's Lemma} \\
&\leq \sqrt{\frac{2log\Pi_G(m)}{m}} \tag{as $|G_{|S}| \leq \Pi_G(m)$}
\end{align}
where it is simple to see that norm of any $g(x_i) \leq \sqrt{m}$.
\end{proof}

This result can now be used with Theorem \ref{th:rademacher_bounds_binary_classification} to provide generalization bounds
with respect to the growth function.

Unfortunately, despite all efforts we've made, the computation of $\Pi_G(m)$ may
not always be convenient because it must be computed for all $m \geq 1$. Instead, in the
next chapters, we will introduce combinatorial measures that are easier to compute and have a direct relation with growth functions.
\end{document}
